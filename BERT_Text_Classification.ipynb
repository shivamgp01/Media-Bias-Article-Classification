{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtLNKsklb2gf"
      },
      "source": [
        "# **BERT Text Classification for Media Bias Detection**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FGO9-XvzrUS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooa07c43zvK4"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUmz93oVbPrA"
      },
      "outputs": [],
      "source": [
        "# Changed for testing each train/validation split\n",
        "VAL_SPLIT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY5ndLBmzw48"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "regular_punct = list(string.punctuation)\n",
        "\n",
        "def remove_punctuation(text, punct_list):\n",
        "  for punc in punct_list:\n",
        "    if punc in text:\n",
        "      text = text.replace(punc, ' ')\n",
        "    return text.strip()\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=\"dataset_train_val.csv\")\n",
        "\n",
        "texts = dataset[\"train\"][\"text\"]\n",
        "for text in texts :\n",
        "  text = remove_punctuation(text, regular_punct)\n",
        "\n",
        "labels = dataset[\"train\"][\"label\"]\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=VAL_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tg1pSU80MdX"
      },
      "outputs": [],
      "source": [
        "print(train_labels[0])\n",
        "print(train_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"csv\", data_files=\"dataset_test.csv\")\n",
        "\n",
        "test_texts = dataset[\"train\"][\"text\"]\n",
        "for text in test_texts :\n",
        "  text = remove_punctuation(text, regular_punct)\n",
        "\n",
        "text_labels = dataset[\"train\"][\"label\"]"
      ],
      "metadata": {
        "id": "eZoKIADx9r_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbX_od7XbdRZ"
      },
      "outputs": [],
      "source": [
        "print(\"Train Dataset Size:\", len(train_texts))\n",
        "print(\"Validation Dataset Size:\", len(val_texts))\n",
        "print(\"Test Dataset Size:\", len(test_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO_8wY810WKI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eviUzjA00Eb"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er8o5-A703Mr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_labels)\n",
        "val_dataset = Dataset(val_encodings, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FLaDwmA5zmQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=3\n",
        ")\n",
        "\n",
        "# Tuned value for each train/val split\n",
        "model.config.dropout = 0.5\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj2Knejp9qKi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEjHWrp75opO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=5e-6,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6cpBM2jrR4N"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S7NubWlrQT8"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqadrThrs4IV"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "test_dataset = Dataset(test_encodings, text_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWle70Mh6HqI"
      },
      "outputs": [],
      "source": [
        "print(trainer.evaluate(train_dataset))\n",
        "print(trainer.evaluate(val_dataset))\n",
        "print(trainer.evaluate(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WKXrCO1arbK"
      },
      "outputs": [],
      "source": [
        "!pip install sigopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME2Kncoxptjn"
      },
      "outputs": [],
      "source": [
        "def sigopt_hp_space(trial):\n",
        "    return [\n",
        "        {\n",
        "            \"bounds\": {\"min\": 1e-6, \"max\": 1e-4},\n",
        "            \"name\": \"learning_rate\",\n",
        "            \"type\": \"double\"\n",
        "        },\n",
        "        {\n",
        "            \"categorical_values\": [\"1\", \"2\", \"4\"],\n",
        "            \"name\": \"per_device_train_batch_size\",\n",
        "            \"type\": \"categorical\"\n",
        "        },\n",
        "        {\n",
        "            \"bounds\": {\"min\": 0.01, \"max\": 0.1},\n",
        "            \"name\": \"weight_decay\",\n",
        "            \"type\": \"double\"\n",
        "        }\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98PYDRdHqtJL"
      },
      "outputs": [],
      "source": [
        "def model_init(trial):\n",
        "  return AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=3\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUJfngerrAlg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    model_init=model_init\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ocpuWSseaB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sigopt import Connection\n",
        "# Need to export SigOpt account client token here to environment variable \"SIGOPT_API_TOKEN\" before creating and running project\n",
        "conn = Connection(client_token=\"INSERT_API_TOKEN\")\n",
        "os.environ['SIGOPT_API_TOKEN'] = \"INSERT_API_TOKEN\"\n",
        "!sigopt create project --project 'huggingface'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8V--3ZkqfAk"
      },
      "outputs": [],
      "source": [
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"sigopt\",\n",
        "    hp_space=sigopt_hp_space,\n",
        "    n_trials=5,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}