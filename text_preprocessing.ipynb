{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffe595b-5664-4c7d-9318-4918e4cac7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# from https://jon-dagdagan.medium.com/fake-news-detection-pre-processing-text-d9648a2854e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b880f97-fc2e-4988-96c2-28f3ab787e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb35c4d6-6dfa-4ce0-a64c-4d70ded64090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import newspaper\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d2710b-0761-4404-92cd-771b534be0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('texts_and_labels_notcleaned.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dcb4c9f-1ec1-4bd2-857c-98330f8eaddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Matt O'Brien\\n\\nAssociated Press\\n\\nYouTube is...</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1597</td>\n",
       "      <td>Colorado students walked out of an event bille...</td>\n",
       "      <td>gun-control</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1600</td>\n",
       "      <td>Mainstream media outlets continue to print fal...</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1602</td>\n",
       "      <td>During the months-long Republican effort to te...</td>\n",
       "      <td>international-politics-and-world-news</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>1608</td>\n",
       "      <td>WASHINGTON – New Mexico Rep. Debra Haaland got...</td>\n",
       "      <td>gender</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1609</td>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>middle-class</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  Matt O'Brien\\n\\nAssociated Press\\n\\nYouTube is...   \n",
       "1              1  FRISCO, Texas — The increasingly bitter disput...   \n",
       "2              2  Speaking to the country for the first time fro...   \n",
       "3              3  A professor who teaches climate change classes...   \n",
       "4              4  The left has a thing for taking babies hostage...   \n",
       "...          ...                                                ...   \n",
       "1005        1597  Colorado students walked out of an event bille...   \n",
       "1006        1600  Mainstream media outlets continue to print fal...   \n",
       "1007        1602  During the months-long Republican effort to te...   \n",
       "1008        1608  WASHINGTON – New Mexico Rep. Debra Haaland got...   \n",
       "1009        1609  As a self-described Democratic socialist, Sen....   \n",
       "\n",
       "                                      label    bias  \n",
       "0                            elections-2020  center  \n",
       "1                                     sport    left  \n",
       "2                               immigration    left  \n",
       "3                               environment   right  \n",
       "4                                  abortion   right  \n",
       "...                                     ...     ...  \n",
       "1005                            gun-control   right  \n",
       "1006                      white-nationalism   right  \n",
       "1007  international-politics-and-world-news    left  \n",
       "1008                                 gender  center  \n",
       "1009                           middle-class   right  \n",
       "\n",
       "[1010 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e78707-3104-4f91-9077-392b012d4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 'News_Category_Dataset_v3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c77f051-8c6c-4447-bb72-d29cbcef6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_info = []\n",
    "# for line in open(file, 'r'):\n",
    "#     article_info.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c9af39-da8f-4e75-a770-59a9c68c7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9',\n",
       " 'headline': 'Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters',\n",
       " 'category': 'U.S. NEWS',\n",
       " 'short_description': 'Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.',\n",
       " 'authors': 'Carla K. Johnson, AP',\n",
       " 'date': '2022-09-23'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9eacf15-cd7b-4710-8087-4c8c11780061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_info_short = article_info[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d9601-4dda-4c1b-88a7-62cdf1fe206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = []\n",
    "# fails = []\n",
    "# for article in article_info_short:\n",
    "#     url = article['link']\n",
    "#     try:\n",
    "#         url_i = newspaper.Article(url=\"%s\" % (url), language='en')\n",
    "#         url_i.download()\n",
    "#         url_i.parse()\n",
    "#         article_texts.append(url_i.text)\n",
    "#         categories.append(article['category'])\n",
    "#     except:\n",
    "#         fails.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c7f10-1c68-4804-b4a6-0d00aec9f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_articles = pd.DataFrame(\n",
    "#     {'article_text': article_texts,\n",
    "#      'category': categories,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af589a42-d5d5-4226-bf5f-ed0a02580e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_articles.to_csv('unprocessed_50k_HuffpostClassification.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a138f234-6ac2-4fe4-b97e-6c6afa454cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jessica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jessica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jessica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jessica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "nltk.download('words') #download list of english words\n",
    "nltk.download('stopwords') #download list of stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopWords = stopwords.words('english')\n",
    "englishWords = set(nltk.corpus.words.words())\n",
    "\n",
    "new_stopwords = [\"associate press\", \"reuters\"]\n",
    "stopWords.extend(new_stopwords)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltkToWordnet(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    pos_tags = nltk.pos_tag(tokens)    \n",
    "    res_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        tag = nltkToWordnet(tag)    \n",
    "        if tag is None:                        \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return res_words\n",
    "\n",
    "def remove_stopWords(tokens):\n",
    "    return [w for w in tokens if (w in englishWords and w not in stopWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386d4a5-3d1e-4539-8c28-b43418e26377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # directory\n",
    "# text_dir = os.path.join(os.getcwd(), 'labeled')\n",
    "# # change depending on your text source\n",
    "# texts = np.load(os.path.join(text_dir, 'docs_text.npy'))\n",
    "\n",
    "# # article is the article\n",
    "# article = texts[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a5d00-157b-42c1-93d1-721c78a81559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb3939-75c9-45af-9974-dde8df1296b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "212a012b-fb14-44a2-8f3a-2627e9a7d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Remove some author/publisher info\n",
    "    if len(text.split(\"(AP) —\")) > 1:\n",
    "        text = \"(AP) —\".join(text.split(\"(AP) —\")[1:])\n",
    "    if len(text.split(\"___\")) > 1:\n",
    "        text = \"___\".join(text.split(\"___\")[:-1])\n",
    "    if len(text.split(\"Associated Press writers\")) > 1:\n",
    "        text = \"Associated Press writers\".join(text.split(\"Associated Press writers\")[:-1])\n",
    "    if len(text.split(\"(Reuters)\")) > 1:\n",
    "        text = \"(Reuters)\".join(text.split(\"(Reuters)\")[1:])\n",
    "    if len(text.split(\"Reporting by\")) > 1:\n",
    "        text = \"Reporting by\".join(text.split(\"Reporting by\")[:-1])\n",
    "    # remove all text in quotes\n",
    "    for x in re.finditer('“(.*?)”', text):\n",
    "        text = re.sub(x.group(), '', text)\n",
    "    # remove digits\n",
    "    text = text.translate(str.maketrans('', '', digits))\n",
    "    # Lowercase and remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text.lower())\n",
    "    # Remove \"advertisement\"\n",
    "    text = re.sub('advertisement', '', text.lower())\n",
    "    # Split up contractions\n",
    "    text = ' '.join([contractions.fix(word) for word in text.split()])\n",
    "    # Tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Lemmatize tokens\n",
    "    tokens = lemmatize(tokens)\n",
    "    # Remove stopwords\n",
    "    tokens = remove_stopWords(tokens)\n",
    "    # Print results\n",
    "    result = ' '.join(tokens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad732650-e037-4a3a-b27b-d96ef9eb9e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['text'][315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28d2ac69-f810-410c-9419-2466ab2eaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = []\n",
    "count = 0\n",
    "for article in articles['text']:\n",
    "    try:\n",
    "        result = process_text(article)\n",
    "        processed_texts.append(result)\n",
    "    except:\n",
    "        processed_texts.append(\"error\")\n",
    "    # print(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501ce8a-1b9c-486b-9ed7-37e207946c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['column_name'].value_counts()[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c89c63f-c75b-441a-9a5d-a5333caf112f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts.count('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e80235d8-9b37-4350-aee5-198a2f1bcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['text'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7a43fb9-48f7-44cc-98a4-f85276954c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv('texts_and_labels_cleaned.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cfb32-0ac2-4246-98c2-56f2b5881506",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_articles_processed = pd.DataFrame(\n",
    "    {'article_text': processed_texts,\n",
    "     'category': categories,\n",
    "    })\n",
    "\n",
    "category_articles_processed.to_csv('processed_50k_HuffpostClassification.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d70198-05d9-429c-a64f-796be518c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = article\n",
    "\n",
    "# # Remove some author/publisher info\n",
    "# if len(text.split(\"(AP) —\")) > 1:\n",
    "#     text = \"(AP) —\".join(text.split(\"(AP) —\")[1:])\n",
    "# if len(text.split(\"___\")) > 1:\n",
    "#     text = \"___\".join(text.split(\"___\")[:-1])\n",
    "# if len(text.split(\"Associated Press writers\")) > 1:\n",
    "#     text = \"Associated Press writers\".join(text.split(\"Associated Press writers\")[:-1])\n",
    "# if len(text.split(\"(Reuters)\")) > 1:\n",
    "#     text = \"(Reuters)\".join(text.split(\"(Reuters)\")[1:])\n",
    "# if len(text.split(\"Reporting by\")) > 1:\n",
    "#     text = \"Reporting by\".join(text.split(\"Reporting by\")[:-1])\n",
    " \n",
    "\n",
    "# # remove all text in quotes\n",
    "# for x in re.finditer('“(.*?)”', text):\n",
    "#     text = re.sub(x.group(), '', text)\n",
    "    \n",
    "# # remove digits\n",
    "# text = text.translate(str.maketrans('', '', digits))\n",
    "\n",
    "# # Lowercase and remove URLs\n",
    "# text = re.sub(r'http\\S+', '', text.lower())\n",
    "# # Remove \"advertisement\"\n",
    "# text = re.sub('advertisement', '', text.lower())\n",
    "\n",
    "# # Split up contractions\n",
    "# text = ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "# # Tokenize text\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# # Lemmatize tokens\n",
    "# tokens = lemmatize(tokens)\n",
    "\n",
    "# # Remove stopwords\n",
    "# tokens = remove_stopWords(tokens)\n",
    "\n",
    "# # Print results\n",
    "# # result = ' '.join(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EECS_545",
   "language": "python",
   "name": "eecs_545"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
