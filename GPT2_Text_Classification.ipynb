{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oaNu3HJtTTo"
      },
      "source": [
        "# **BERT Text Classification for Media Bias Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUz_vmR8tyoc"
      },
      "source": [
        "# Install transformers library and ml-things toolkit\n",
        "!pip install transformers\n",
        "!pip install ml-things"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjr_J342tOPq"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (set_seed,\n",
        "                          TrainingArguments,\n",
        "                          Trainer,\n",
        "                          GPT2Config,\n",
        "                          GPT2Tokenizer,\n",
        "                          AdamW, \n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          GPT2ForSequenceClassification)\n",
        "\n",
        "\n",
        "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
        "epochs = 5\n",
        "\n",
        "# Number of batches - depending on the max sequence length and GPU memory.\n",
        "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
        "# For small sequence length can try batch of 32 or higher.\n",
        "batch_size = 1\n",
        "\n",
        "# Pad or truncate text sequences to a specific length\n",
        "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
        "max_length = 1024\n",
        "\n",
        "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Name of transformers model - will use already pretrained model.\n",
        "# Path of transformer model - will load your own model from local disk.\n",
        "model_name_or_path = 'gpt2'\n",
        "\n",
        "# Dictionary of labels and their id - this will be used to convert.\n",
        "# String labels to number ids.\n",
        "labels_ids = {'left': 0, 'center': 1, 'right': 2}\n",
        "\n",
        "# How many labels are we using in training.\n",
        "# This is used to decide size of classification head.\n",
        "n_labels = len(labels_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_SPLIT = 0.4"
      ],
      "metadata": {
        "id": "GxLzPUnT52tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDEubgJIt23C"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, path, use_tokenizer, is_train):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    text_labels = list(zip(df['text'].tolist(), df['category'].tolist()))\n",
        "\n",
        "    if is_train :\n",
        "      N = int((1-VAL_SPLIT)*len(text_labels))\n",
        "    else :\n",
        "      N = len(text_labels)\n",
        "    samples_text_labels = random.sample(text_labels, N)\n",
        "    self.texts, self.labels = list(zip(*samples_text_labels))\n",
        "\n",
        "    print(self.labels[0])\n",
        "    print(self.texts[0])\n",
        "    \n",
        "    # Set number of exmaples\n",
        "    self.n_examples = len(self.labels)\n",
        "    return\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return {'text':str(self.texts[item]),\n",
        "            'label':str(self.labels[item])}\n",
        "\n",
        "class Gpt2ClassificationCollator(object):\n",
        "    r\"\"\"\n",
        "    Data Collator used for GPT2 in a classificaiton rask. \n",
        "    \n",
        "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
        "    can go straight into a GPT2 model.\n",
        "\n",
        "    This class is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
        "          Transformer type tokenizer used to process raw text into numbers.\n",
        "\n",
        "      labels_ids (:obj:`dict`):\n",
        "          Dictionary to encode any labels names into numbers. Keys map to \n",
        "          labels names and Values map to number associated to those labels.\n",
        "\n",
        "      max_sequence_len (:obj:`int`, `optional`)\n",
        "          Value to indicate the maximum desired sequence to truncate or pad text\n",
        "          sequences. If no value is passed it will used maximum sequence size\n",
        "          supported by the tokenizer and model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
        "        # Tokenizer to be used inside the class.\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "        # Check max sequence length.\n",
        "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "        # Label encoder used inside the class.\n",
        "        self.labels_encoder = labels_encoder\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        r\"\"\"\n",
        "        This function allowes the class objesct to be used as a function call.\n",
        "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
        "        class as a function.\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "          item (:obj:`list`):\n",
        "              List of texts and labels.\n",
        "\n",
        "        Returns:\n",
        "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
        "          It holddes the statement `model(**Returned Dictionary)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get all texts from sequences list.\n",
        "        texts = [sequence['text'] for sequence in sequences]\n",
        "        # Get all labels from sequences list.\n",
        "        labels = [sequence['label'] for sequence in sequences]\n",
        "        # Encode all labels using label encoder.\n",
        "        labels = [self.labels_encoder[label] for label in labels]\n",
        "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
        "        # appropriate padding.\n",
        "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "        # Update the inputs with the associated encoded labels as tensor.\n",
        "        inputs.update({'labels':torch.tensor(labels)})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "def train(dataloader, optimizer_, scheduler_, device_):\n",
        "  r\"\"\"\n",
        "  Train pytorch model on a single pass through the data loader.\n",
        "\n",
        "  It will use the global variable `model` which is the transformer model \n",
        "  loaded on `_device` that we want to train on.\n",
        "\n",
        "  This function is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
        "          Parsed data into batches of tensors.\n",
        "\n",
        "      optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
        "          Optimizer used for training.\n",
        "\n",
        "      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
        "          PyTorch scheduler.\n",
        "\n",
        "      device_ (:obj:`torch.device`):\n",
        "          Device used to load tensors before feeding to model.\n",
        "\n",
        "  Returns:\n",
        "\n",
        "      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
        "        Labels, Train Average Loss].\n",
        "  \"\"\"\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables.\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  # Total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model into training mode.\n",
        "  model.train()\n",
        "\n",
        "  # For each batch of training data...\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # Add original labels - use later for evaluation.\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "    \n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "    \n",
        "    # Always clear any previously calculated gradients before performing a\n",
        "    # backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Perform a forward pass (evaluate the model on this training batch).\n",
        "    # This will return the loss (rather than the model output) because we\n",
        "    # have provided the `labels`.\n",
        "    # The documentation for this a bert model function is here: \n",
        "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    # The call to `model` always returns a tuple, so we need to pull the \n",
        "    # loss value out of the tuple along with the logits. We will use logits\n",
        "    # later to calculate training accuracy.\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Accumulate the training loss over all of the batches so that we can\n",
        "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "    # single value; the `.item()` function just returns the Python value \n",
        "    # from the tensor.\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Perform a backward pass to calculate the gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters and take a step using the computed gradient.\n",
        "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "    # modified based on their gradients, the learning rate, etc.\n",
        "    optimizer_.step()\n",
        "\n",
        "    # Update the learning rate.\n",
        "    scheduler_.step()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Convert these logits to list of predicted labels values.\n",
        "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "  \n",
        "  # Return all true labels and prediction for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss\n",
        "\n",
        "def validation(dataloader, device_):\n",
        "  r\"\"\"Validation function to evaluate model performance on a \n",
        "  separate set of data.\n",
        "\n",
        "  This function will return the true and predicted labels so we can use later\n",
        "  to evaluate the model's performance.\n",
        "\n",
        "  This function is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
        "          Parsed data into batches of tensors.\n",
        "\n",
        "    device_ (:obj:`torch.device`):\n",
        "          Device used to load tensors before feeding to model.\n",
        "\n",
        "  Returns:\n",
        "    \n",
        "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
        "        Labels, Train Average Loss]\n",
        "  \"\"\"\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  #total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # add original labels\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "\n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up validation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have\n",
        "        # not provided labels.\n",
        "        # token_type_ids is the same as the \"segment ids\", which \n",
        "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple along with the logits. We will use logits\n",
        "        # later to to calculate training accuracy.\n",
        "        loss, logits = outputs[:2]\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # get predicitons to list\n",
        "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "        # update list\n",
        "        predictions_labels += predict_content\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "\n",
        "  # Return all true labels and prediciton for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctTxnW903_VF"
      },
      "source": [
        "# Get model configuration.\n",
        "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
        "\n",
        "# Get model's tokenizer.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
        "# default to left padding\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Define PAD Token = EOS Token = 50256\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the actual model.\n",
        "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
        "\n",
        "# resize model embedding to match new tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# fix model padding token id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Load model to defined device.\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlXROUWu5Osq"
      },
      "source": [
        "import random\n",
        "\n",
        "# Create data collator to encode text and labels into numbers.\n",
        "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
        "                                                          labels_encoder=labels_ids, \n",
        "                                                          max_sequence_len=max_length)\n",
        "\n",
        "# Create pytorch dataset.\n",
        "train_dataset = TextDataset(path=\"dataset_train_val.csv\", \n",
        "                               use_tokenizer=tokenizer, is_train=True)\n",
        "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
        "\n",
        "# Move pytorch dataset into dataloader.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
        "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
        "\n",
        "# Create pytorch dataset.\n",
        "valid_dataset =  TextDataset(path='dataset_test.csv', \n",
        "                               use_tokenizer=tokenizer, is_train=False)\n",
        "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
        "\n",
        "# Move pytorch dataset into dataloader.\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
        "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLPCkiv-lis3"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
        "# us the number of batches.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "all_acc = {'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "# Loop through each epoch.\n",
        "print('Epoch')\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print()\n",
        "  print('Training on batches...')\n",
        "  # Perform one full pass over the training set.\n",
        "  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
        "  train_acc = accuracy_score(train_labels, train_predict)\n",
        "\n",
        "  # Get prediction form model on validation data. \n",
        "  print('Validation on batches...')\n",
        "  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
        "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
        "\n",
        "  # Print loss and accuracy values to see how training evolves.\n",
        "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
        "  print()\n",
        "\n",
        "  # Store the loss value for plotting the learning curve.\n",
        "  all_loss['train_loss'].append(train_loss)\n",
        "  all_loss['val_loss'].append(val_loss)\n",
        "  all_acc['train_acc'].append(train_acc)\n",
        "  all_acc['val_acc'].append(val_acc)\n",
        "\n",
        "# Plot loss curves.\n",
        "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
        "\n",
        "# Plot accuracy curves.\n",
        "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sifp6ocoSng"
      },
      "source": [
        "# Get prediction form model on validation data. This is where you should use\n",
        "# your test data.\n",
        "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
        "\n",
        "# Create the evaluation report.\n",
        "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
        "# Show the evaluation report.\n",
        "print(evaluation_report)\n",
        "\n",
        "# Plot confusion matrix.\n",
        "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
        "                      classes=list(labels_ids.keys()), normalize=True, \n",
        "                      magnify=0.1,\n",
        "                      );"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WKXrCO1arbK"
      },
      "outputs": [],
      "source": [
        "!pip install sigopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME2Kncoxptjn"
      },
      "outputs": [],
      "source": [
        "def sigopt_hp_space(trial):\n",
        "    return [\n",
        "        {\n",
        "            \"bounds\": {\"min\": 1e-6, \"max\": 1e-4},\n",
        "            \"name\": \"learning_rate\",\n",
        "            \"type\": \"double\"\n",
        "        },\n",
        "        {\n",
        "            \"categorical_values\": [\"1\", \"2\", \"4\"],\n",
        "            \"name\": \"per_device_train_batch_size\",\n",
        "            \"type\": \"categorical\"\n",
        "        },\n",
        "        {\n",
        "            \"bounds\": {\"min\": 0.01, \"max\": 0.1},\n",
        "            \"name\": \"weight_decay\",\n",
        "            \"type\": \"double\"\n",
        "        }\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98PYDRdHqtJL"
      },
      "outputs": [],
      "source": [
        "def model_init(trial):\n",
        "  return GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUJfngerrAlg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    model_init=model_init\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ocpuWSseaB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sigopt import Connection\n",
        "# Need to export SigOpt account client token here to environment variable \"SIGOPT_API_TOKEN\" before creating and running project\n",
        "conn = Connection(client_token=\"INSERT_API_TOKEN\")\n",
        "os.environ['SIGOPT_API_TOKEN'] = \"INSERT_API_TOKEN\"\n",
        "!sigopt create project --project 'huggingface'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8V--3ZkqfAk"
      },
      "outputs": [],
      "source": [
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"sigopt\",\n",
        "    hp_space=sigopt_hp_space,\n",
        "    n_trials=5,\n",
        ")"
      ]
    }
  ]
}